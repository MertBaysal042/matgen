# MatGen - Parallel Sparse Matrix Scaling and Value Estimation

A high-performance C library for generating sparse matrices through parallel scaling algorithms (Nearest Neighbor and Bilinear Interpolation) with realistic value estimation. Implements OpenMP, MPI, and CUDA backends for scalability.

## ğŸ“š Research Foundation

This project implements algorithms from the following research:

1. **Agarwal, A., Dahleh, M., Shah, D., and Shen, D. (2021)**
   _Causal Matrix Completion_
   arXiv:2109.15154v1
   - Matrix completion techniques for sparse matrices
   - Value estimation and prediction methods

2. **Bruch, S., Nardini, F. M., Rulli, C., and Venturini, R. (2025)**
   _Efficient Sketching and Nearest Neighbor Search Algorithms for Sparse Vector Sets_
   arXiv:2509.24815v1
   - Efficient nearest neighbor algorithms for sparse data
   - Distance computations and similarity metrics

3. **MatGen Framework by Ali Emre Pamuk**
   - Sparse matrix generation methodology

## ğŸ¯ Project Goals

Generate new sparse matrices by **scaling existing ones** using two interpolation methods:

### 1. Nearest Neighbor Scaling

- Maps each output position to its nearest input position
- Preserves exact sparsity (output has at most input nnz)
- Fast, simple, suitable for discrete data
- Parallel implementations: OpenMP, MPI, CUDA

### 2. Bilinear Interpolation Scaling

- Weighted average of 4 neighboring input positions
- Smooth interpolation with sparsity control
- Can densify output (controlled by threshold)
- Parallel implementations: OpenMP, MPI, CUDA

### 3. Realistic Value Estimation

- Predict values for newly interpolated positions
- Learn distributions from real sparse matrices
- Maintain statistical properties during scaling

### 4. Extensions

- Non-square (rectangular) matrix scaling
- Parallel structural feature extraction
- Matrix quality validation

## ğŸ—ï¸ Project Structure

```
matgen/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/                    # Core sparse matrix formats
â”‚   â”‚   â”œâ”€â”€ coo.c               # Coordinate format
â”‚   â”‚   â”œâ”€â”€ csr.c               # Compressed Sparse Row
â”‚   â”‚   â””â”€â”€ conversion.c        # Format conversion
â”‚   â”‚
â”‚   â”œâ”€â”€ io/                      # Input/Output
â”‚   â”‚   â”œâ”€â”€ mtx_reader.c        # Matrix Market reader
â”‚   â”‚   â””â”€â”€ mtx_writer.c        # Matrix Market writer
â”‚   â”‚
â”‚   â”œâ”€â”€ scaling/                 # Scaling algorithms (sequential)
â”‚   â”‚   â”œâ”€â”€ nearest.c           # Nearest neighbor scaling
â”‚   â”‚   â””â”€â”€ bilinear.c          # Bilinear interpolation scaling
â”‚   â”‚
â”‚   â”œâ”€â”€ parallel/                # Parallel implementations
â”‚   â”‚   â”œâ”€â”€ openmp/             # OpenMP backend
â”‚   â”‚   â”‚   â”œâ”€â”€ scale_nearest_omp.c
â”‚   â”‚   â”‚   â”œâ”€â”€ scale_bilinear_omp.c
â”‚   â”‚   â”‚   â””â”€â”€ omp_utils.c
â”‚   â”‚   â”œâ”€â”€ mpi/                # MPI backend
â”‚   â”‚   â”‚   â”œâ”€â”€ scale_nearest_mpi.c
â”‚   â”‚   â”‚   â”œâ”€â”€ scale_bilinear_mpi.c
â”‚   â”‚   â”‚   â””â”€â”€ mpi_utils.c
â”‚   â”‚   â””â”€â”€ cuda/               # CUDA backend
â”‚   â”‚       â”œâ”€â”€ scale_nearest.cu
â”‚   â”‚       â”œâ”€â”€ scale_bilinear.cu
â”‚   â”‚       â””â”€â”€ cuda_utils.cu
â”‚   â”‚
â”‚   â”œâ”€â”€ values/                  # Value estimation
â”‚   â”‚   â”œâ”€â”€ value_learner.c     # Learn from real matrices
â”‚   â”‚   â”œâ”€â”€ value_estimator.c   # Estimate during scaling
â”‚   â”‚   â””â”€â”€ distributions.c     # Statistical distributions
â”‚   â”‚
â”‚   â”œâ”€â”€ features/                # Feature extraction
â”‚   â”‚   â”œâ”€â”€ degree_dist.c       # Degree distribution
â”‚   â”‚   â”œâ”€â”€ statistics.c        # Matrix statistics
â”‚   â”‚   â””â”€â”€ quality_metrics.c   # Quality evaluation
â”‚   â”‚
â”‚   â””â”€â”€ ops/                     # Matrix operations
â”‚       â”œâ”€â”€ spmv.c              # Sparse matrix-vector multiply
â”‚       â”œâ”€â”€ vector_ops.c        # Dense vector operations
â”‚       â””â”€â”€ distances.c         # Sparse distance metrics
â”‚
â”œâ”€â”€ include/matgen/              # Public API headers
â”œâ”€â”€ tests/                       # Unit tests (GoogleTest)
â”œâ”€â”€ benchmarks/                  # Performance benchmarks
â””â”€â”€ examples/                    # Usage examples
```

## ğŸš€ Implementation Status

### âœ… Completed

- [x] COO and CSR sparse matrix formats
- [x] Format conversion (COO â†” CSR)
- [x] Matrix Market I/O with symmetric expansion
- [x] Sequential nearest neighbor scaling
- [x] Sequential bilinear interpolation scaling
- [x] OpenMP parallel scaling (both methods)
- [x] MPI distributed scaling (broadcast-gather strategy)
- [] CUDA GPU scaling (dynamic sparse output)
- [x] Matrix operations (SpMV, vector ops, distances)

### ğŸ”„ In Progress

- [ ] Testing and validation of all parallel backends
- [ ] Performance benchmarking (strong/weak scaling)
- [ ] CUDA kernel optimization (two-pass algorithm)

### ğŸ“‹ Planned

- [ ] Value estimation implementation
  - [ ] Learn value distributions from real matrices
  - [ ] Statistical models (normal, log-normal, power-law)
  - [ ] Value prediction during interpolation
- [ ] Non-square matrix support
- [ ] Parallel feature extraction
- [ ] Matrix quality metrics
- [ ] Comprehensive benchmarking suite

## ğŸ› ï¸ Building the Project

### Requirements

- **C Compiler**: GCC 9.0+ or Clang 10.0+ (C17)
- **CMake**: 3.25+
- **Ninja**: Recommended build system

**Optional:**

- **OpenMP**: 4.5+ (multi-core parallelism)
- **MPI**: OpenMPI 4.0+ or MPICH 3.3+ (distributed)
- **CUDA**: 11.0+ (GPU acceleration)
- **GoogleTest**: Unit testing
- **GoogleBenchmark**: Performance testing

### Quick Build

```bash
# Configure
cmake --preset windows-msvc-release

# Build
cmake --build --preset windows-msvc-release

# Test
ctest --preset windows-msvc-release
```

### Enable Parallel Backends

```bash
# OpenMP only
cmake --preset windows-msvc-release -DENABLE_OPENMP=ON

# MPI only
cmake --preset windows-msvc-release -DENABLE_MPI=ON

# CUDA only
cmake --preset windows-msvc-release -DENABLE_CUDA=ON

# All backends
cmake --preset windows-msvc-release -DENABLE_OPENMP=ON -DENABLE_MPI=ON -DENABLE_CUDA=ON
```

## ğŸ“Š Usage Examples

### Scale Matrix with Nearest Neighbor (Sequential)

```cpp
#include <matgen/io/mtx_reader.h>
#include <matgen/io/mtx_writer.h>
#include <matgen/scaling/nearest.h>

// Read input matrix (1000x1000)
MatGenCOO* input = matgen_mtx_read_coo("input.mtx");

// Scale to 5000x5000 using nearest neighbor
MatGenCOO* output = matgen_scale_nearest(input, 5000, 5000);

// Write output
matgen_mtx_write_coo("output.mtx", output);

// Cleanup
matgen_coo_destroy(input);
matgen_coo_destroy(output);
```

### Scale with Bilinear Interpolation (OpenMP)

```cpp
#include <matgen/parallel/openmp/scale_bilinear_omp.h>

// Convert to CSR for efficient lookup
MatGenCSR* input_csr = matgen_coo_to_csr(input);

// Scale with 8 threads, sparsity threshold = 0.01
omp_set_num_threads(8);
MatGenCOO* output = matgen_scale_bilinear_omp(input_csr, 5000, 5000, 0.01);

matgen_csr_destroy(input_csr);
```

### Distributed Scaling with MPI

```cpp
#include <matgen/parallel/mpi/scale_nearest_mpi.h>
#include <mpi.h>

int main(int argc, char** argv) {
    MPI_Init(&argc, &argv);

    int rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);

    // Root loads input matrix
    MatGenCOO* input = NULL;
    if (rank == 0) {
        input = matgen_mtx_read_coo("input.mtx");
    }

    // Distributed scaling
    MatGenCOO* output = matgen_scale_nearest_mpi(input, 10000, 10000,
                                                   MPI_COMM_WORLD);

    // Root writes result
    if (rank == 0) {
        matgen_mtx_write_coo("output.mtx", output);
    }

    matgen_coo_destroy(input);
    matgen_coo_destroy(output);

    MPI_Finalize();
    return 0;
}
```

### GPU Scaling with CUDA

```cpp
#include <matgen/parallel/cuda/scale_bilinear.h>

MatGenCSR* input_csr = matgen_coo_to_csr(input);

// Scale on GPU
MatGenCOO* output = matgen_scale_bilinear_cuda(input_csr, 5000, 5000, 0.01);

matgen_csr_destroy(input_csr);
```

## ğŸ“ˆ Performance Goals

- **OpenMP**: Near-linear speedup up to number of physical cores
- **MPI**: Scalability to 100+ processes for large matrices
- **CUDA**: 10-100x speedup over sequential for large matrices

## ğŸ§ª Testing

```bash
# Run all tests
ctest --preset windows-msvc-release -V

# Run specific test suite
./out/build/windows-msvc-release/tests/test_scaling
```
